# The promise of multi-agent systems.
###### _5 minute read._ Written by Prasenjit Day, Ashish Jagmohan, and Ravi Kokku. 

## What are multi-agent systems?

With the rise of large language model (LLM)-based agents, the landscape of artificial intelligence is witnessing a significant transformation, as discussed in our last blog post.

This development paves the way for multi-agent systems (MAS), where collaboration between agents enables complex problem-solving capabilities with minimal human interference. 

These types of systems allow for the accomplishment of much more complex tasks than those that can be achieved by a single autonomous agent. For example, a multi-agent system can design, build, and test a new computer program (entirely autonomously) by deploying several agents taking on different specialized roles involved in the software development lifecycle.

Our focus now shifts to the immense potential of MAS, which fuels the work of Emergence’s engineers and researchers. This discussion aims to unpack the capabilities and applications of MAS in the era of advanced AI.

## Current Examples

### [ChatDev](https://github.com/OpenBMB/ChatDev)

This framework essentially creates a software development company of agents. These agents assume different roles seen in a typical software development organization, and they cooperate to create a tested software.

### [AutoGen](https://github.com/microsoft/autogen)

AutoGen provides a framework for communication between agents. These agents are able to collaborate on and refine the instructions for a task until its completion. 

This framework includes the existence of a User Proxy Agent which initiates tasks on behalf of the user’s interests. It also includes an Assistant Agent, which provides code and Chain-Of-Thought reasoning to other agents. 

The mark of completion of a task in AutoGen is the successful execution of the code generated by the Assistant Agent or the executive of some maximum number of steps. 

AutoGen also supports multi-agent communication, whereby all inter-agent messages are sent through an Agent Manager agent.

### [OpenAGI](https://github.com/agiresearch/OpenAGI)

Given a goal, OpenAGI provides a complete framework to plan sub-tasks, execute sub-tasks, and evaluate and improve itself. 

The user provides the definition of a task and the set of Domain Expert Models from various repositories for OpenAGI to rely on. The system then completes the task independently, self-evaluating against a ground-truth dataset. 

This self-evaluation can be fed through the inbuilt RLTF (Reinforcement Learning through Task Feedback) loop to make the planning LLM at the core of OpenAGI more accurate with time.

_One disadvantage of this framework is that the user has to provide the domain expert model list at the start of the execution for this to work. The framework cannot decide which set of tools or domain expert models should it use to get the task done._

### [JARVIS](https://github.com/microsoft/JARVIS)

JARVIS is a realization of the [paper HuggingGPT](https://arxiv.org/pdf/2303.17580.pdf), which combines OpenAI models with HuggingFace domain expert models to create a system capable of complex planning and sub-task realization and execution. 

It actually independently decides which set of Domain Expert Models from Hugging Face should be used for sub-task execution. 

_It does not, however, provide an evaluation tool bench like OpenAGI’s which could be used to improve its Planner agent model through RLTF._  

### [ToolBench](https://github.com/OpenBMB/ToolBench)

The ToolBench framework was designed to provide a data-generation and evaluation system by which a model, such as LLaMa, could be fine-tuned toward accurately and independently finding APIs which it could use to solve a problem.  

There are two stages of data generation which are engaged by ToolBench. In the first stage, an agent generates instructions and a set of APIs (taken from RapidAPI) that are relevant to the instruction. In the second stage, a series of actions are generated based on the instruction, and an API subset is selected for best completion of the sub-tasks defined by the generated actions. 

ToolBench uses ChatGPT to evaluate itself and its actions.

## Limitations and Prospects

While there has been significant progress in using large language models to create multi-agent systems, the efficacy and consistency of these systems undeniably leave more to be desired.

This is largely due to remaining gaps in the capabilities of LLMs. To read about these gaps and how one might mitigate them, read our next blog post.
